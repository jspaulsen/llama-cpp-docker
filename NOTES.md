```
LLAMA_ARG_THREADS - "number of threads to use during generation (default: -1)"
LLAMA_ARG_CTX_SIZE - "size of the prompt context (default: 4096, 0 = loaded from model)"
LLAMA_ARG_N_PREDICT - "number of tokens to predict (default: -1, -1 = infinity)"
LLAMA_ARG_BATCH - "logical maximum batch size (default: 2048)"
LLAMA_ARG_UBATCH - "physical maximum batch size (default: 512)"
LLAMA_ARG_FLASH_ATTN - "enable Flash Attention (default: disabled)"
LLAMA_ARG_NO_PERF - "disable internal libllama performance timings (default: false)"
LLAMA_ARG_ROPE_SCALING_TYPE - "RoPE frequency scaling method, defaults to linear unless specified by the model"
LLAMA_ARG_ROPE_SCALE - "RoPE context scaling factor, expands context by a factor of N"
LLAMA_ARG_ROPE_FREQ_BASE - "RoPE base frequency, used by NTK-aware scaling (default: loaded from model)"
LLAMA_ARG_ROPE_FREQ_SCALE - "RoPE frequency scaling factor, expands context by a factor of 1/N"
LLAMA_ARG_YARN_ORIG_CTX - "YaRN: original context size of model (default: 0 = model training context size)"
LLAMA_ARG_YARN_EXT_FACTOR - "YaRN: extrapolation mix factor (default: -1.0, 0.0 = full interpolation)"
LLAMA_ARG_YARN_ATTN_FACTOR - "YaRN: scale sqrt(t) or attention magnitude (default: 1.0)"
LLAMA_ARG_YARN_BETA_SLOW - "YaRN: high correction dim or alpha (default: 1.0)"
LLAMA_ARG_YARN_BETA_FAST - "YaRN: low correction dim or beta (default: 32.0)"
LLAMA_ARG_NO_KV_OFFLOAD - "disable KV offload"
LLAMA_ARG_CACHE_TYPE_K - "KV cache data type for K (allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1; default: f16)"
LLAMA_ARG_CACHE_TYPE_V - "KV cache data type for V (allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1; default: f16)"
LLAMA_ARG_DEFRAG_THOLD - "KV cache defragmentation threshold (default: 0.1, < 0 - disabled)"
LLAMA_ARG_N_PARALLEL - "number of parallel sequences to decode (default: 1)"
LLAMA_ARG_MLOCK - "force system to keep model in RAM rather than swapping or compressing"
LLAMA_ARG_NO_MMAP - "do not memory-map model (slower load but may reduce pageouts if not using mlock)"
LLAMA_ARG_NUMA - "attempt optimizations that help on some NUMA systems. Options: 'distribute' (spread execution evenly over all nodes), 'isolate' (only spawn threads on CPUs on the node that execution started on), 'numactl' (use the CPU map provided by numactl). It is recommended to drop the system page cache before using this if run without it previously."
LLAMA_ARG_DEVICE - "comma-separated list of devices to use for offloading (none = don't offload). Use --list-devices to see a list of available devices."
LLAMA_ARG_N_GPU_LAYERS - "number of layers to store in VRAM"
LLAMA_ARG_SPLIT_MODE - "how to split the model across multiple GPUs. Options: 'none' (use one GPU only), 'layer' (default, split layers and KV across GPUs), 'row' (split rows across GPUs)."
LLAMA_ARG_TENSOR_SPLIT - "fraction of the model to offload to each GPU, comma-separated list of proportions, e.g. 3,1"
LLAMA_ARG_MAIN_GPU - "the GPU to use for the model (with split-mode = none), or for intermediate results and KV (with split-mode = row) (default: 0)"
LLAMA_ARG_MODEL - "model path (default: models/$filename with filename from --hf-file or --model-url if set, otherwise models/7B/ggml-model-f16.gguf)"
LLAMA_ARG_MODEL_URL - "model download url (default: unused)"
LLAMA_ARG_HF_REPO - "Hugging Face model repository; quant is optional, case-insensitive, default to Q4_K_M, or falls back to the first file in the repo if Q4_K_M doesn't exist. mmproj is also downloaded automatically if available. to disable, add --no-mmproj (default: unused)"
LLAMA_ARG_HFD_REPO - "Same as --hf-repo, but for the draft model (default: unused)"
LLAMA_ARG_HF_FILE - "Hugging Face model file. If specified, it will override the quant in --hf-repo (default: unused)"
LLAMA_ARG_HF_REPO_V - "Hugging Face model repository for the vocoder model (default: unused)"
LLAMA_ARG_HF_FILE_V - "Hugging Face model file for the vocoder model (default: unused)"
HF_TOKEN - "Hugging Face access token"
LLAMA_LOG_COLORS - "Enable colored logging"
LLAMA_LOG_VERBOSITY - "Set the verbosity threshold. Messages with a higher verbosity will be ignored."
LLAMA_LOG_PREFIX - "Enable prefix in log messages"
LLAMA_LOG_TIMESTAMPS - "Enable timestamps in log messages"
LLAMA_ARG_NO_CONTEXT_SHIFT - "disables context shift on infinite text generation (default: disabled)"
LLAMA_ARG_POOLING - "pooling type for embeddings (none,mean,cls,last,rank), use model default if unspecified"
LLAMA_ARG_CONT_BATCHING - "enable continuous batching (a.k.a dynamic batching) (default: enabled)"
LLAMA_ARG_NO_CONT_BATCHING - "disable continuous batching"
LLAMA_ARG_MMPROJ - "path to a multimodal projector file. See tools/mtmd/README.md. Note: if -hf is used, this argument can be omitted."
LLAMA_ARG_MMPROJ_URL - "URL to a multimodal projector file. See tools/mtmd/README.md."
LLAMA_ARG_NO_MMPROJ - "explicitly disable multimodal projector, useful when using -hf"
LLAMA_ARG_NO_MMPROJ_OFFLOAD - "do not offload multimodal projector to GPU"
LLAMA_ARG_ALIAS - "set alias for model name (to be used by REST API)"
LLAMA_ARG_HOST - "ip address to listen, or bind to an UNIX socket if the address ends with .sock (default: 127.0.0.1)"
LLAMA_ARG_PORT - "port to listen (default: 8080)"
LLAMA_ARG_STATIC_PATH - "path to serve static files from (default: )"
LLAMA_ARG_NO_WEBUI - "Disable the Web UI (default: enabled)"
LLAMA_ARG_EMBEDDINGS - "restrict to only support embedding use case; use only with dedicated embedding models (default: disabled)"
LLAMA_ARG_RERANKING - "enable reranking endpoint on server (default: disabled)"
LLAMA_API_KEY - "API key to use for authentication (default: none)"
LLAMA_ARG_SSL_KEY_FILE - "path to file a PEM-encoded SSL private key"
LLAMA_ARG_SSL_CERT_FILE - "path to file a PEM-encoded SSL certificate"
LLAMA_ARG_TIMEOUT - "server read/write timeout in seconds (default: 600)"
LLAMA_ARG_THREADS_HTTP - "number of threads used to process HTTP requests (default: -1)"
LLAMA_ARG_CACHE_REUSE - "min chunk size to attempt reusing from the cache via KV shifting (default: 0)"
LLAMA_ARG_ENDPOINT_METRICS - "enable prometheus compatible metrics endpoint (default: disabled)"
LLAMA_ARG_ENDPOINT_SLOTS - "enable slots monitoring endpoint (default: disabled)"
LLAMA_ARG_ENDPOINT_PROPS - "enable changing global properties via POST /props (default: disabled)"
LLAMA_ARG_NO_ENDPOINT_SLOTS - "disables slots monitoring endpoint"
LLAMA_ARG_JINJA - "use jinja template for chat (default: disabled)"
LLAMA_ARG_THINK - "controls whether thought tags are allowed and/or extracted from the response, and in which format they're returned. Options: 'none' (leaves thoughts unparsed in message.content), 'deepseek' (puts thoughts in message.reasoning_content, except in streaming mode which behaves as none). Default: deepseek."
LLAMA_ARG_THINK_BUDGET - "controls the amount of thinking allowed; currently only one of: -1 for unrestricted thinking budget, or 0 to disable thinking (default: -1)"
LLAMA_ARG_CHAT_TEMPLATE - "set custom jinja chat template (default: template taken from model's metadata). If suffix/prefix are specified, template will be disabled. Only commonly used templates are accepted (unless --jinja is set before this flag). See README for list of built-in templates."
LLAMA_ARG_CHAT_TEMPLATE_FILE - "set custom jinja chat template file (default: template taken from model's metadata). If suffix/prefix are specified, template will be disabled. Only commonly used templates are accepted (unless --jinja is set before this flag). See README for list of built-in templates."
LLAMA_ARG_NO_PREFILL_ASSISTANT - "whether to prefill the assistant's response if the last message is an assistant message (default: prefill enabled). When this flag is set, if the last message is an assistant message then it will be treated as a full message and not prefilled."
LLAMA_ARG_DRAFT_MAX - "number of tokens to draft for speculative decoding (default: 16)"
LLAMA_ARG_DRAFT_MIN - "minimum number of draft tokens to use for speculative decoding (default: 0)"
LLAMA_ARG_DRAFT_P_MIN - "minimum speculative decoding probability (greedy) (default: 0.8)"
LLAMA_ARG_CTX_SIZE_DRAFT - "size of the prompt context for the draft model (default: 0, 0 = loaded from model)"
LLAMA_ARG_N_GPU_LAYERS_DRAFT - "number of layers to store in VRAM for the draft model"
LLAMA_ARG_MODEL_DRAFT - "draft model for speculative decoding (default: unused)"
```
